{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cc9e43",
   "metadata": {},
   "source": [
    "# PHYS805 Final Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "import mplhep as hep\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# auto reload of imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import data_utils\n",
    "from utils import dataloader\n",
    "from utils import metric\n",
    "from utils import model\n",
    "from utils import training\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b5945",
   "metadata": {},
   "source": [
    "Setting up the data. This includes selecting the features, loading the data from the `.root` files specified in the configuration file, and constructing the data loaders using the `JetDataClass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40211d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets.yaml\", \"r\") as f:\n",
    "    ds_cfg = yaml.safe_load(f)\n",
    "\n",
    "features = [\n",
    "    \"Jet_pt\",\n",
    "    \"Jet_eta\",\n",
    "    \"Jet_phi\",\n",
    "    \"Jet_mass\",\n",
    "    \"Jet_nConstituents\",\n",
    "    \"Jet_nSVs\",\n",
    "    \"Jet_area\",\n",
    "]\n",
    "other_branches = [\n",
    "    \"nJet\",\n",
    "    \"Pileup_nPU\",\n",
    "]\n",
    "branches = features + other_branches\n",
    "num_ftrs = len(features)\n",
    "\n",
    "test_split = 0.2\n",
    "val_split = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = data_utils.load_data(ds_cfg, \"EMJ\", filter_name=branches, entry_stop=-1)\n",
    "bkg = data_utils.load_data(ds_cfg, \"QCD\", filter_name=branches, entry_stop=100_000)\n",
    "bkg = data_utils.match_pu(sig, bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f072da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nPU = torch.tensor(\n",
    "    np.concatenate([\n",
    "        ak.to_numpy(sig[\"Pileup_nPU\"]),\n",
    "        ak.to_numpy(bkg[\"Pileup_nPU\"])\n",
    "    ]),\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# njets = int(max(ak.max(sig[\"nJet\"]), ak.max(bkg[\"nJet\"])))\n",
    "njets = 10\n",
    "sig_tensor = data_utils.ak_to_torch(sig, features, njets, label=1)\n",
    "bkg_tensor = data_utils.ak_to_torch(bkg, features, njets, label=0)\n",
    "data_tensor = torch.cat([sig_tensor, bkg_tensor], dim=0)\n",
    "rnd_idx = torch.randperm(data_tensor.size(0))\n",
    "data_tensor = data_tensor[rnd_idx]\n",
    "nPU = nPU[rnd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    data_tensor[..., :-1],\n",
    "    data_tensor[:, 0, -1],\n",
    "    test_size=test_split,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# test and val split\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size= val_split\n",
    ")\n",
    "\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_constants = data_utils.compute_norm_constants(X_train)\n",
    "train_ds = dataloader.JetDataset(X_train, norm_constants, y_train)\n",
    "val_ds = dataloader.JetDataset(X_val, norm_constants, y_val)\n",
    "test_ds = dataloader.JetDataset(X_test, norm_constants, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(norm_constants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee871f",
   "metadata": {},
   "source": [
    "## Features Study\n",
    "\n",
    "In this section, we explore the features selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79730512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot njets distribution\n",
    "fig, ax = plt.subplots()\n",
    "sig_njets = ak.to_numpy(sig['nJet'])\n",
    "bkg_njets = ak.to_numpy(bkg['nJet'])\n",
    "ax.hist(sig_njets, bins=np.arange(0, 15), density=False, label=\"EMJ\", histtype=\"step\")\n",
    "ax.hist(bkg_njets, bins=np.arange(0, 15), density=False, label=\"QCD\", histtype=\"step\")\n",
    "ax.set_xlabel(\"Number of Jets\")\n",
    "ax.set_ylabel(\"A.U.\")\n",
    "ax.set_title(\"Jet Multiplicity Distribution\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6352bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pileup distribution\n",
    "bins = 20\n",
    "x_min = 0\n",
    "x_max = nPU.max().item() + 1\n",
    "\n",
    "# Plotting PU before resampling\n",
    "sig_pu = data_utils.load_data(ds_cfg, \"EMJ\", filter_name=\"Pileup_nPU\", entry_stop=-1)[\"Pileup_nPU\"].to_numpy()\n",
    "bkg_pu = data_utils.load_data(ds_cfg, \"QCD\", filter_name=\"Pileup_nPU\", entry_stop=-1)[\"Pileup_nPU\"].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(sig_pu, bins=bins, range=(x_min, x_max), label=\"EMJ\", histtype=\"step\", density=True)\n",
    "ax.hist(bkg_pu, bins=bins, range=(x_min, x_max), label=\"QCD\", histtype=\"step\", density=True)\n",
    "ax.set_xlabel(\"Pileup\")\n",
    "ax.set_ylabel(\"A.U.\")\n",
    "ax.set_title(\"Pileup Distributions Before Resampling\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "del(bkg_pu, sig_pu)\n",
    "\n",
    "# After resampling\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(ak.to_numpy(sig[\"Pileup_nPU\"]), bins=bins, range=(x_min, x_max), density=True, label=\"EMJ\", histtype=\"step\")\n",
    "ax.hist(ak.to_numpy(bkg[\"Pileup_nPU\"]), bins=bins, range=(x_min, x_max), density=True, label=\"QCD\", histtype=\"step\")\n",
    "ax.set_xlabel(\"Number of Pileup\")\n",
    "ax.set_title(\"Pileup Distributions After Resampling\")\n",
    "ax.set_ylabel(\"A.U.\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_njets = ak.to_numpy(sig['nJet'])\n",
    "bkg_njets = ak.to_numpy(bkg['nJet'])\n",
    "sig_jets = sig[sig_njets < njets]\n",
    "bkg_jets = bkg[bkg_njets < njets]\n",
    "\n",
    "def plot_ftrs(sig, bkg, ftr_name, fig_title, nbins, xrange, xlabel, ylabel, log, normalized=True):\n",
    "    sig_ftr = sig[ftr_name]\n",
    "    bkg_ftr = bkg[ftr_name]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(ak.flatten(sig_ftr), bins=nbins, range=xrange, density=normalized, alpha=0.5, label='EMJ', histtype='step', linewidth=1.5)\n",
    "    ax.hist(ak.flatten(bkg_ftr), bins=nbins, range=xrange, density=normalized, alpha=0.5, label='QCD', histtype='step', linewidth=1.5)\n",
    "    ax.set_title(fig_title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_yscale('log' if log else 'linear')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687df949",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_pt\", \"Jet $p_T$\", 20, (0, 3500), \"Jet $p_T$ [GeV]\", \"Count\", log=True, normalized=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_eta\", \"Jet $\\eta$\", 20, (-5, 5), \"Jet $\\eta$\", \"Count\", log=False, normalized=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_phi\", \"Jet $\\phi$\", 20, (-3.14, 3.14), \"Jet $\\phi$\", \"Count\", log=False, normalized=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_mass\", \"Jet Mass\", 20, (0, 300), \"Jet Mass [GeV]\", \"Count\", log=True, normalized=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_nConstituents\", \"Jet Number of Constituents\", 20, (0, 100), \"Number of Constituents\", \"Count\", log=True, normalized=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_nSVs\", \"Jet Number of Secondary Vertices\", 10, (0, 10), \"Number of Secondary Vertices\", \"Count\", log=True, normalized=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_area\", \"Jet Area\", 30, (0, 1), \"Jet Area\", \"Count\", log=True, normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at feature correlation with PU\n",
    "lead_jets = data_tensor[:, 0, :]\n",
    "corr_coefs = {}\n",
    "for i, ftr_name in enumerate(features):\n",
    "    corr_matrix = torch.corrcoef(torch.stack((lead_jets[:, i], nPU), dim=0))\n",
    "    corr_coefs[ftr_name] = corr_matrix[0, 1].item()\n",
    "    print(f\"Correlation coefficient between {ftr_name} and nPU: {corr_coefs[ftr_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa5383",
   "metadata": {},
   "source": [
    "## Baseline: Training NN on Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a13d5a",
   "metadata": {},
   "source": [
    "We now setup and train a baseline model which consists of a simple feed-forward NN. This same architecture is used later on, but with the input dimension appropriately adapted to the expected inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6068148",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_nn = {\n",
    "    \"n_epochs\": 500,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_heads\": 2,\n",
    "    \"num_layers\": 1,\n",
    "    \"hidden_size\": 4,\n",
    "    \"beta\": 0.5,\n",
    "    \"patience\": 20,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "pprint(config_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metric.metrics()\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "classifier_nn = model.MLPClassifier(\n",
    "    input_dim=len(features) * njets,\n",
    "    hidden_dim=config_nn[\"hidden_size\"],\n",
    "    hidden_layers=2,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "optimizer_nn = torch.optim.Adam(\n",
    "    list(classifier_nn.parameters()),\n",
    "    lr=config_nn[\"learning_rate\"]\n",
    ")\n",
    "    \n",
    "early_stopper = training.EarlyStopping(config_nn[\"patience\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a15ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier train without encoder\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "wandb.init(\n",
    "    project=\"ml4phys_finalproj\",\n",
    "    name=f\"run_classifier_nn_baseline_{timestamp}\",\n",
    "    config=config_nn\n",
    ")\n",
    "\n",
    "early_stopper = training.EarlyStopping(patience=config_nn[\"patience\"]) # New instance\n",
    "\n",
    "for epoch in range(config_nn[\"n_epochs\"]):\n",
    "    metrics.reset()\n",
    "    classifier_nn.train()\n",
    "    correct = count = 0\n",
    "    total_bce = 0.0\n",
    "    for x, _, labels in train_loader:\n",
    "        x = x.to(device)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        logit = classifier_nn(x)\n",
    "\n",
    "        loss_bce = bce_loss(logit.squeeze(), labels)\n",
    "        optimizer_nn.zero_grad()\n",
    "        loss_bce.backward()\n",
    "        optimizer_nn.step()\n",
    "        total_bce += loss_bce.item() * x.size(0)\n",
    "        metrics.update(labels.cpu(), (sigmoid(logit).squeeze() > 0.5).long().cpu())\n",
    "        \n",
    "    avg_bce = total_bce / len(train_ds)\n",
    "    accuracy, precision, recall, f1_score = metrics.compute()\n",
    "\n",
    "    # Eval\n",
    "    metrics.reset()\n",
    "    correct_val = count_val = 0\n",
    "    total_bce_val = 0.0\n",
    "    classifier_nn.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_val, _, labels_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            x_val = x_val.view(x_val.size(0), -1)\n",
    "            labels_val = labels_val.to(device)\n",
    "            logit_val = classifier_nn(x_val)\n",
    "\n",
    "            loss_bce_val = bce_loss(logit_val.squeeze(), labels_val)\n",
    "            total_bce_val += loss_bce_val.item() * x_val.size(0)\n",
    "            metrics.update(labels_val.cpu(), (sigmoid(logit_val).squeeze() > 0.5).long().cpu())\n",
    "    \n",
    "    avg_bce_val = total_bce_val / len(val_ds)\n",
    "    accuracy_val, precision_val, recall_val, f1_score_val = metrics.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config_nn['n_epochs']}\")\n",
    "    print(f\"Loss: {avg_bce:.4f}, BCE: {avg_bce:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Val Loss: {avg_bce_val:.4f}, Val BCE: {avg_bce_val:.4f}, Val Accuracy: {accuracy_val:.4f}, Val Precision: {precision_val:.4f}, Val Recall: {recall_val:.4f}, Val F1 Score: {f1_score_val:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "\n",
    "        \"bce_loss\": avg_bce,\n",
    "        \"val_bce_loss\": avg_bce_val,\n",
    "\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"val_accuracy\": accuracy_val,\n",
    "        \"val_precision\": precision_val,\n",
    "        \"val_recall\": recall_val,\n",
    "        \"val_f1_score\": f1_score_val,\n",
    "    })\n",
    "\n",
    "    if early_stopper(avg_bce_val, classifier_nn):\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc943cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from best checkpoint\n",
    "classifier_nn.load_state_dict(early_stopper.best_model_state)\n",
    "\n",
    "# Save best model \n",
    "torch.save(classifier_nn.state_dict(), \"./models/classifier_nn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaadaabc",
   "metadata": {},
   "source": [
    "## Jointly Trained Encoder + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c1011",
   "metadata": {},
   "source": [
    "This first approach trains the transformer encoder and classifier jointly as a single model. It uses as the loss the linear combination of the Supervised Contrastive Loss and the BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_joint = {\n",
    "    \"n_epochs\": 500,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_heads\": 2,\n",
    "    \"num_layers\": 1,\n",
    "    \"hidden_size\": 4,\n",
    "    \"contrastive_temp\": 0.07,\n",
    "    \"beta\": 0.5,\n",
    "    \"patience\": 20,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "pprint(config_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3766a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metric.metrics()\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "contrastive_loss = training.SupConLoss(config_joint[\"contrastive_temp\"])\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "encoder_joint = model.TransformerEncoder(\n",
    "    num_features=num_ftrs,\n",
    "    embed_size=config_joint[\"hidden_size\"],\n",
    "    num_heads=config_joint[\"num_heads\"],\n",
    "    num_layers=config_joint[\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "classifier_joint = model.MLPClassifier(\n",
    "    input_dim=config_joint[\"hidden_size\"],\n",
    "    hidden_dim=config_joint[\"hidden_size\"],\n",
    "    hidden_layers=2,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "optimizer_joint = torch.optim.Adam(\n",
    "    list(encoder_joint.parameters()) + list(classifier_joint.parameters()),\n",
    "    lr=config_joint[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "early_stopper = training.EarlyStopping(config_joint[\"patience\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e073192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "wandb.init(\n",
    "    project=\"ml4phys_finalproj\",\n",
    "    name=f\"run_jointarch_{timestamp}\",\n",
    "    config=config_joint\n",
    ")\n",
    "\n",
    "for epoch in range(config_joint[\"n_epochs\"]):\n",
    "    metrics.reset()\n",
    "    encoder_joint.train()\n",
    "    classifier_joint.train()\n",
    "\n",
    "    correct = count = 0\n",
    "    total_loss = total_bce = total_contrastive = 0.0\n",
    "    for x, mask, labels in train_loader:\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        latent = encoder_joint(x, mask).to(device)\n",
    "        logit = classifier_joint(latent)\n",
    "\n",
    "        loss_bce = bce_loss(logit.squeeze(), labels)\n",
    "        loss_contrastive = contrastive_loss(latent.unsqueeze(1), labels)\n",
    "        loss = (1 - config_joint[\"beta\"]) * loss_bce + config_joint[\"beta\"] * loss_contrastive\n",
    "        optimizer_joint.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_joint.step()\n",
    "\n",
    "        total_contrastive += loss_contrastive.item() * x.size(0)\n",
    "        total_bce += loss_bce.item() * x.size(0)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        metrics.update(labels.cpu(), (sigmoid(logit).squeeze() > 0.5).long().cpu())\n",
    "        \n",
    "    avg_loss = total_loss / len(train_ds)\n",
    "    avg_contrastive = total_contrastive / len(train_ds)\n",
    "    avg_bce = total_bce / len(train_ds)\n",
    "    accuracy, precision, recall, f1_score = metrics.compute()\n",
    "\n",
    "    # Eval\n",
    "    metrics.reset()\n",
    "    correct_val = count_val = 0\n",
    "    total_loss_val = total_bce_val = total_contrastive_val = 0.0\n",
    "    encoder_joint.eval()\n",
    "    classifier_joint.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_val, mask_val, labels_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "\n",
    "            latent_val = encoder_joint(x_val, mask_val).to(device)\n",
    "            logit_val = classifier_joint(latent_val)\n",
    "\n",
    "            loss_contrastive_val = contrastive_loss(latent_val.unsqueeze(1), labels_val)\n",
    "            loss_bce_val = bce_loss(logit_val.squeeze(), labels_val)\n",
    "            loss_val = (1 - config_joint[\"beta\"]) * loss_bce_val + config_joint[\"beta\"] * loss_contrastive_val\n",
    "            total_contrastive_val += loss_contrastive_val.item() * x_val.size(0)\n",
    "            total_bce_val += loss_bce_val.item() * x_val.size(0)\n",
    "            total_loss_val += loss_val.item() * x_val.size(0)\n",
    "        metrics.update(labels_val.cpu(), (sigmoid(logit_val).squeeze() > 0.5).long().cpu())\n",
    "\n",
    "    avg_contrastive_val = total_contrastive_val / len(val_ds)\n",
    "    avg_bce_val = total_bce_val / len(val_ds)\n",
    "    avg_loss_val = total_loss_val / len(val_ds)\n",
    "    accuracy_val, precision_val, recall_val, f1_score_val = metrics.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config_joint['n_epochs']}\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, BCE: {avg_bce:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Val Loss: {avg_loss_val:.4f}, Val BCE: {avg_bce_val:.4f}, Val Accuracy: {accuracy_val:.4f}, Val Precision: {precision_val:.4f}, Val Recall: {recall_val:.4f}, Val F1 Score: {f1_score_val:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "\n",
    "        \"contrastive_loss\": avg_contrastive,\n",
    "        \"bce_loss\": avg_bce,\n",
    "        \"loss\": avg_loss,\n",
    "        \"val_contrastive_loss\": avg_contrastive_val,\n",
    "        \"val_bce_loss\": avg_bce_val,\n",
    "        \"val_loss\": avg_loss_val,\n",
    "\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"val_accuracy\": accuracy_val,\n",
    "        \"val_precision\": precision_val,\n",
    "        \"val_recall\": recall_val,\n",
    "        \"val_f1_score\": f1_score_val,\n",
    "    })\n",
    "\n",
    "    if early_stopper(avg_loss_val, [encoder_joint, classifier_joint]):\n",
    "        break\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from early stopping\n",
    "best_model_states = early_stopper.best_model_state\n",
    "encoder_joint.load_state_dict(best_model_states[0])\n",
    "classifier_joint.load_state_dict(best_model_states[1])\n",
    "\n",
    "# Save\n",
    "torch.save({\n",
    "    \"encoder_state\": encoder_joint.state_dict(),\n",
    "    \"classifier_state\": classifier_joint.state_dict(),\n",
    "}, \"./models/encoder_classifier_joint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda8c28",
   "metadata": {},
   "source": [
    "## Encoder + Classifier w/ Encoder Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674e450",
   "metadata": {},
   "source": [
    "This second approach is the same as the one above, but the models are now trained separately: first the encoder is trained using the Supervised Constrastive Loss, then this component is frozen, and a small classifier NN is trained on the embeddings of this encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_pretr = {\n",
    "    \"n_epochs_encoder\": 500,\n",
    "    \"n_epochs_classifier\": 500,\n",
    "    \"learning_rate_encoder\": 1e-3,\n",
    "    \"learning_rate_classifier\": 1e-3,\n",
    "    \"num_heads\": 2,\n",
    "    \"num_layers\": 1,\n",
    "    \"hidden_size\": 4,\n",
    "    \"contrastive_temp\": 0.07,\n",
    "    \"patience_encoder\": 20,\n",
    "    \"patience_classifier\": 20,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "pprint(config_pretr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metric.metrics()\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "contrastive_loss = training.SupConLoss(config_pretr[\"contrastive_temp\"])\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "classifier = model.MLPClassifier(\n",
    "    input_dim=config_pretr[\"hidden_size\"],\n",
    "    hidden_dim=config_pretr[\"hidden_size\"],\n",
    "    hidden_layers=2,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "optimizer_classifier = torch.optim.Adam(\n",
    "    classifier.parameters(),\n",
    "    lr=config_pretr[\"learning_rate_classifier\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06505b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.TransformerEncoder(\n",
    "    num_features=num_ftrs,\n",
    "    embed_size=config_pretr[\"hidden_size\"],\n",
    "    num_heads=config_pretr[\"num_heads\"],\n",
    "    num_layers=config_pretr[\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer_encoder = torch.optim.Adam(\n",
    "    encoder.parameters(),\n",
    "    lr=config_pretr[\"learning_rate_encoder\"]\n",
    ")\n",
    "\n",
    "# Encoder pretrain\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "wandb.init(\n",
    "    project=\"ml4phys_finalproj\",\n",
    "    name=f\"run_encoderpretrain_{timestamp}\",\n",
    "    config=config_pretr\n",
    ")\n",
    "\n",
    "early_stopper = training.EarlyStopping(config_pretr[\"patience_encoder\"])\n",
    "for epoch in range(config_pretr[\"n_epochs_encoder\"]):\n",
    "    encoder.train()\n",
    "    total_contrastive = 0.0\n",
    "    for x, mask, labels in train_loader:\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        latent = encoder(x, mask).to(device)\n",
    "\n",
    "        loss_contrastive = contrastive_loss(latent.unsqueeze(1), labels)\n",
    "        optimizer_encoder.zero_grad()\n",
    "        loss_contrastive.backward()\n",
    "        optimizer_encoder.step()\n",
    "        total_contrastive += loss_contrastive.item() * x.size(0)\n",
    "        \n",
    "    avg_contrastive = total_contrastive / len(train_ds)\n",
    "\n",
    "    # Eval\n",
    "    encoder.eval()\n",
    "    total_contrastive_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_val, mask_val, labels_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "\n",
    "            latent_val = encoder(x_val, mask_val).to(device)\n",
    "            loss_contrastive_val = contrastive_loss(latent_val.unsqueeze(1), labels_val)\n",
    "            total_contrastive_val += loss_contrastive_val.item() * x_val.size(0)\n",
    "\n",
    "    avg_contrastive_val = total_contrastive_val / len(val_ds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config_pretr['n_epochs_encoder']}\")\n",
    "    print(f\"Contrastive Loss: {avg_contrastive:.4f}, Contrastive Loss Val: {avg_contrastive_val:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"contrastive_loss\": avg_contrastive,\n",
    "        \"val_contrastive_loss\": avg_contrastive_val,\n",
    "    })\n",
    "    \n",
    "    if early_stopper(avg_contrastive_val, encoder):\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from the early stopping\n",
    "encoder.load_state_dict(early_stopper.best_model_state)\n",
    "\n",
    "# Save best model \n",
    "# torch.save(encoder.state_dict(), \"./models/encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier train with pretrained encoder\n",
    "wandb.init(\n",
    "    project=\"ml4phys_finalproj\",\n",
    "    name=f\"run_classifier_wpretrainedencoder_{timestamp}\",\n",
    "    config=config_pretr\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "early_stopper = training.EarlyStopping(patience=config_pretr[\"patience_classifier\"]) # New instance\n",
    "\n",
    "for epoch in range(config_pretr[\"n_epochs_classifier\"]):\n",
    "    metrics.reset()\n",
    "    classifier.train()\n",
    "    correct = count = 0\n",
    "    total_bce = 0.0\n",
    "    for x, mask, labels in train_loader:\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        latent = encoder(x, mask).to(device)\n",
    "        logit = classifier(latent)\n",
    "\n",
    "        loss_bce = bce_loss(logit.squeeze(), labels)\n",
    "        optimizer_classifier.zero_grad()\n",
    "        loss_bce.backward()\n",
    "        optimizer_classifier.step()\n",
    "        total_bce += loss_bce.item() * x.size(0)\n",
    "        metrics.update(labels.cpu(), (sigmoid(logit).squeeze() > 0.5).long().cpu())\n",
    "        \n",
    "    avg_bce = total_bce / len(train_ds)\n",
    "    accuracy, precision, recall, f1_score = metrics.compute()\n",
    "\n",
    "    # Eval\n",
    "    metrics.reset()\n",
    "    correct_val = count_val = 0\n",
    "    total_bce_val = 0.0\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_val, mask_val, labels_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "            latent_val = encoder(x_val, mask_val).to(device)\n",
    "            logit_val = classifier(latent_val)\n",
    "\n",
    "            loss_bce_val = bce_loss(logit_val.squeeze(), labels_val)\n",
    "            total_bce_val += loss_bce_val.item() * x_val.size(0)\n",
    "            metrics.update(labels_val.cpu(), (sigmoid(logit_val).squeeze() > 0.5).long().cpu())\n",
    "    \n",
    "    avg_bce_val = total_bce_val / len(val_ds)\n",
    "    accuracy_val, precision_val, recall_val, f1_score_val = metrics.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config_pretr['n_epochs_classifier']}\")\n",
    "    print(f\"Loss: {avg_bce:.4f}, BCE: {avg_bce:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Val Loss: {avg_bce_val:.4f}, Val BCE: {avg_bce_val:.4f}, Val Accuracy: {accuracy_val:.4f}, Val Precision: {precision_val:.4f}, Val Recall: {recall_val:.4f}, Val F1 Score: {f1_score_val:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "\n",
    "        \"bce_loss\": avg_bce,\n",
    "        \"val_bce_loss\": avg_bce_val,\n",
    "\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"val_accuracy\": accuracy_val,\n",
    "        \"val_precision\": precision_val,\n",
    "        \"val_recall\": recall_val,\n",
    "        \"val_f1_score\": f1_score_val,\n",
    "    })\n",
    "\n",
    "    if early_stopper(avg_bce_val, classifier):\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a467edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from best checkpoint\n",
    "classifier.load_state_dict(early_stopper.best_model_state)\n",
    "\n",
    "# Save best model \n",
    "torch.save({\n",
    "    \"encoder_state\": encoder.state_dict(),\n",
    "    \"classifier_state\": classifier.state_dict(),    \n",
    "}, \"./models/encoder_classifier_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd80ea",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473a1e3",
   "metadata": {},
   "source": [
    "The models are re-instantiated, and the saved states are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee350513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading models\n",
    "# Baseline NN\n",
    "classifier_nn = model.MLPClassifier(\n",
    "    input_dim=len(features) * njets,\n",
    "    hidden_dim=config_nn[\"hidden_size\"],\n",
    "    hidden_layers=2,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "classifier_nn.load_state_dict(torch.load(\"./models/classifier_nn.pth\"))\n",
    "\n",
    "# Joint architecture\n",
    "checkpoint_joint = torch.load(\"./models/encoder_classifier_joint.pth\")\n",
    "encoder_joint = model.TransformerEncoder(\n",
    "    num_features=num_ftrs,\n",
    "    embed_size=config_joint[\"hidden_size\"],\n",
    "    num_heads=config_joint[\"num_heads\"],\n",
    "    num_layers=config_joint[\"num_layers\"]\n",
    ").to(device)\n",
    "encoder_joint.load_state_dict(checkpoint_joint[\"encoder_state\"])\n",
    "\n",
    "classifier_joint = model.MLPClassifier(\n",
    "    input_dim=config_joint[\"hidden_size\"],\n",
    "    hidden_dim=config_joint[\"hidden_size\"],\n",
    "    hidden_layers=2,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "classifier_joint.load_state_dict(checkpoint_joint[\"classifier_state\"])\n",
    "\n",
    "# Pretrained encoder + classifier\n",
    "checkpoint_pretr = torch.load(\"./models/encoder_classifier_pretrained.pth\")\n",
    "encoder_pretr = model.TransformerEncoder(\n",
    "    num_features=num_ftrs,\n",
    "    embed_size=config_pretr[\"hidden_size\"],\n",
    "    num_heads=config_pretr[\"num_heads\"],\n",
    "    num_layers=config_pretr[\"num_layers\"]\n",
    ").to(device)\n",
    "encoder_pretr.load_state_dict(checkpoint_pretr[\"encoder_state\"])\n",
    "\n",
    "classifier_pretr = model.MLPClassifier(\n",
    "    input_dim=config_pretr[\"hidden_size\"],\n",
    "    hidden_dim=config_pretr[\"hidden_size\"],\n",
    "    hidden_layers=2,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "classifier_pretr.load_state_dict(checkpoint_pretr[\"classifier_state\"])\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80a7d5",
   "metadata": {},
   "source": [
    "We now evaluate the trained models on the previously untouched test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_and_logits(encoder, classifier, dataloader):\n",
    "    metrics = metric.metrics()\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    all_embeddings = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, mask, labels in dataloader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            latent = encoder(x, mask).to(device)\n",
    "            logit = classifier(latent)\n",
    "\n",
    "            all_embeddings.append(latent.cpu())\n",
    "            all_logits.append(logit.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "            print(logit)\n",
    "\n",
    "            metrics.update(labels.cpu().numpy(), sigmoid(logit).float().cpu())\n",
    "\n",
    "    print(metrics.preds)\n",
    "    print(metrics.labels)\n",
    "    accuracy, precision, recall, f1_score = metrics.compute()\n",
    "    return torch.cat(all_embeddings), torch.cat(all_logits), torch.cat(all_labels)\n",
    "\n",
    "print(\"Performance metrics for jointly trained model...\")\n",
    "embeddings_joint, logits_joint, labels_joint = get_embeddings_and_logits(encoder_joint, classifier_joint, test_loader)\n",
    "print(\"Performance metrics for pretrained encoder + classifier model...\")\n",
    "embeddings_pretr, logits_pretr, labels_pretr = get_embeddings_and_logits(encoder_pretr, classifier_pretr, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logits for baseline NN\n",
    "def get_logits_nn(classifier, dataloader):\n",
    "    classifier.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, _, labels in dataloader:\n",
    "            x = x.to(device)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logit = classifier(x)\n",
    "\n",
    "            all_logits.append(logit.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    return torch.cat(all_logits), torch.cat(all_labels)\n",
    "\n",
    "logits_nn, labels_nn = get_logits_nn(classifier_nn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_joint = torch.sigmoid(logits_joint).squeeze().numpy()\n",
    "prob_pretr = torch.sigmoid(logits_pretr).squeeze().numpy()\n",
    "prob_nn = torch.sigmoid(logits_nn).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tensor = test_ds.data.to(device)\n",
    "test_data_labels = test_ds.labels.to(device)\n",
    "\n",
    "# Get PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_joint = pca.fit_transform(embeddings_joint.cpu().numpy())\n",
    "pca_pretr = pca.fit_transform(embeddings_pretr.cpu().numpy())\n",
    "\n",
    "embeddings_joint_df = pd.DataFrame({\n",
    "    \"label\": test_data_labels.cpu().numpy(),\n",
    "    \"embed_1\": embeddings_joint[0:, 0].cpu().numpy(),\n",
    "    \"embed_2\": embeddings_joint[0:, 1].cpu().numpy(),\n",
    "    \"embed_3\": embeddings_joint[0:, 2].cpu().numpy(),\n",
    "    \"embed_4\": embeddings_joint[0:, 3].cpu().numpy(),\n",
    "    \"PCA_1\": pca_joint[:, 0],\n",
    "    \"PCA_2\": pca_joint[:, 1],\n",
    "    \"PCA_3\": pca_joint[:, 2],\n",
    "    \"PCA_4\": pca_joint[:, 3],\n",
    "})\n",
    "embeddings_pretr_df = pd.DataFrame({\n",
    "    \"label\": test_data_labels.cpu().numpy(),\n",
    "    \"embed_1\": embeddings_pretr[0:, 0].cpu().numpy(),\n",
    "    \"embed_2\": embeddings_pretr[0:, 1].cpu().numpy(),\n",
    "    \"embed_3\": embeddings_pretr[0:, 2].cpu().numpy(),\n",
    "    \"embed_4\": embeddings_pretr[0:, 3].cpu().numpy(),\n",
    "    \"PCA_1\": pca_pretr[:, 0],\n",
    "    \"PCA_2\": pca_pretr[:, 1],\n",
    "    \"PCA_3\": pca_pretr[:, 2],\n",
    "    \"PCA_4\": pca_pretr[:, 3],\n",
    "})\n",
    "\n",
    "# Convert labels to str (EJ = 1, QCD = 0)\n",
    "embeddings_joint_df[\"label\"] = embeddings_joint_df[\"label\"].map({1: \"EJ\", 0: \"QCD\"})\n",
    "embeddings_pretr_df[\"label\"] = embeddings_pretr_df[\"label\"].map({1: \"EJ\", 0: \"QCD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    embeddings_joint_df,\n",
    "    vars=[\"PCA_1\", \"PCA_2\", \"PCA_3\", \"PCA_4\"],\n",
    "    hue=\"label\",\n",
    "    diag_kind=\"hist\",\n",
    "    plot_kws={\"alpha\": 0.5},\n",
    "    diag_kws={\"bins\": 30, \"alpha\": 0.5}\n",
    ")\n",
    "plt.suptitle(\"Joint Model Embeddings PCA Pairplot\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c91f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    embeddings_pretr_df,\n",
    "    vars=[\"PCA_1\", \"PCA_2\", \"PCA_3\", \"PCA_4\"],\n",
    "    hue=\"label\",\n",
    "    diag_kind=\"hist\",\n",
    "    plot_kws={\"alpha\": 0.5},\n",
    "    diag_kws={\"bins\": 30, \"alpha\": 0.5}\n",
    ")\n",
    "plt.suptitle(\"Pretrained Encoder Model Embeddings PCA Pairplot\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3fea9",
   "metadata": {},
   "source": [
    "ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1aae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_joint, tpr_joint, _ = roc_curve(labels_joint.numpy(), torch.sigmoid(logits_joint).squeeze().numpy())\n",
    "fpr_pretr, tpr_pretr, _ = roc_curve(labels_pretr.numpy(), torch.sigmoid(logits_pretr).squeeze().numpy())\n",
    "fpr_nn, tpr_nn, _ = roc_curve(labels_nn.numpy(), torch.sigmoid(logits_nn).squeeze().numpy())\n",
    "\n",
    "auc_joint = auc(fpr_joint, tpr_joint)\n",
    "auc_pretr = auc(fpr_pretr, tpr_pretr)\n",
    "auc_nn = auc(fpr_nn, tpr_nn)\n",
    "\n",
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tpr_joint, 1 - fpr_joint, label=f\"Joint AUC: {auc_joint:.2f}\")\n",
    "ax.plot(tpr_pretr, 1 - fpr_pretr, label=f\"Pretrained AUC: {auc_pretr:.2f}\")\n",
    "ax.plot(tpr_nn, 1 - fpr_nn, label=f\"NN Baseline AUC: {auc_nn:.2f}\")\n",
    "# Diagonal line\n",
    "ax.plot([1, 0], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "ax.set_xlabel(\"Signal Efficiency\")\n",
    "ax.set_ylabel(\"Background Rejection\")\n",
    "ax.set_title(\"ROC Curves on Test Set\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores\n",
    "def plot_scores(logits, labels, title, ylog=False, nbins=50):\n",
    "    scores = torch.sigmoid(logits).squeeze().numpy()\n",
    "    labels = labels.numpy()\n",
    "    sig_scores = scores[labels == 1]\n",
    "    bkg_scores = scores[labels == 0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(sig_scores, bins=nbins, range=(0, 1), density=False, label=\"EJ\", histtype=\"step\", linewidth=1.5)\n",
    "    ax.hist(bkg_scores, bins=nbins, range=(0, 1), density=False, label=\"QCD\", histtype=\"step\", linewidth=1.5)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Classifier Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_yscale('log' if ylog else 'linear')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_scores(logits_joint, labels_joint, \"Joint Model Classifier Scores\", nbins=25)\n",
    "plot_scores(logits_pretr, labels_pretr, \"Pretrained Encoder Model Classifier Scores\", nbins=25)\n",
    "plot_scores(logits_nn, labels_nn, \"NN Baseline Classifier Scores\", nbins=25)\n",
    "\n",
    "plot_scores(logits_joint, labels_joint, \"Joint Model Classifier Scores\", ylog=True, nbins=25)\n",
    "plot_scores(logits_pretr, labels_pretr, \"Pretrained Encoder Model Classifier Scores\", ylog=True, nbins=25)\n",
    "plot_scores(logits_nn, labels_nn, \"NN Baseline Classifier Scores\", ylog=True, nbins=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
