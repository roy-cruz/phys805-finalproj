{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cc9e43",
   "metadata": {},
   "source": [
    "# PHYS805 Final Project Notebook\n",
    "\n",
    "- Overview:\n",
    "    - Supervised\n",
    "        - Use transformer over jets\n",
    "        - Attach classifier head\n",
    "        - Train en-to-end to predict signal vs background using CE loss\n",
    "    - Self-supervised + supervised probe\n",
    "        - Pretrain: \n",
    "            - First train using InfoNCE. \n",
    "            - No labels, no pretext objective. \n",
    "            - Just finding good representation. \n",
    "            - Would NOT use labels here.\n",
    "        - Freeze transformer encoder. Then train small classification head to predict signal vs background. Would use labels here.\n",
    "\n",
    "<!-- ```\n",
    "ml4phys$ eosls /store/group/lpcemj/EMJAnalysis2025\n",
    "QCD_PT-1000to1400_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-100to1400_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-120to170_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-1400to1800_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-15to30_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-170to300_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-1800to2400_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-2400to3200_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-300to470_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-30to50_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-3200_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-470to600_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-50to80_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-600to800_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-800to1000_TuneCP5_13p6TeV_pythia8\n",
    "QCD_PT-80to120_TuneCP5_13p6TeV_pythia8\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mplhep as hep\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# auto reload of imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import data_utils\n",
    "from utils import dataloader\n",
    "from utils import metrics\n",
    "from utils import model\n",
    "from utils import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40211d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config stuff\n",
    "with open(\"datasets.yaml\", \"r\") as f:\n",
    "    ds_cfg = yaml.safe_load(f)\n",
    "\n",
    "features = [\n",
    "    \"Jet_pt\",\n",
    "    \"Jet_eta\",\n",
    "    \"Jet_phi\",\n",
    "    \"Jet_mass\",\n",
    "    \"Jet_nConstituents\",\n",
    "    \"Jet_nSVs\",\n",
    "    \"Jet_area\",\n",
    "]\n",
    "other_branches = [\n",
    "    \"nJet\",\n",
    "    \"Pileup_nPU\",\n",
    "]\n",
    "branches = features + other_branches\n",
    "num_ftrs = len(features)\n",
    "\n",
    "test_split = 0.2\n",
    "val_split = 0.5\n",
    "njets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_events_per_class = 20_000\n",
    "sig = data_utils.load_data(ds_cfg, \"EMJ\", filter_name=branches, entry_stop=max_events_per_class)\n",
    "bkg = data_utils.load_data(ds_cfg, \"QCD\", filter_name=branches, entry_stop=max_events_per_class)\n",
    "\n",
    "nPU = torch.tensor(\n",
    "    np.concatenate([\n",
    "        ak.to_numpy(sig[\"Pileup_nPU\"]),\n",
    "        ak.to_numpy(bkg[\"Pileup_nPU\"])\n",
    "    ]),\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "sig_tensor = data_utils.ak_to_torch(sig, features, njets, label=1)\n",
    "bkg_tensor = data_utils.ak_to_torch(bkg, features, njets, label=0)\n",
    "data_tensor = torch.cat([sig_tensor, bkg_tensor], dim=0)\n",
    "rnd_idx = torch.randperm(data_tensor.size(0))\n",
    "data_tensor = data_tensor[rnd_idx]\n",
    "nPU = nPU[rnd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    data_tensor[..., :-1],\n",
    "    data_tensor[:, 0, -1],\n",
    "    test_size=test_split,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# test and val split\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size= val_split\n",
    ")\n",
    "\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_constants = data_utils.compute_norm_constants(X_train)\n",
    "train_ds = dataloader.JetDataset(X_train, norm_constants, y_train)\n",
    "val_ds = dataloader.JetDataset(X_val, norm_constants, y_val)\n",
    "test_ds = dataloader.JetDataset(X_test, norm_constants, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee871f",
   "metadata": {},
   "source": [
    "## Features Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79730512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot njets distribution\n",
    "fig, ax = plt.subplots()\n",
    "sig_njets = ak.to_numpy(sig['nJet'])\n",
    "bkg_njets = ak.to_numpy(bkg['nJet'])\n",
    "ax.hist(sig_njets, bins=np.arange(0, 15), density=True, label='Signal (EMJ)', histtype='step')\n",
    "ax.hist(bkg_njets, bins=np.arange(0, 15), density=True, label='Background (QCD)', histtype='step')\n",
    "ax.set_xlabel('Number of Jets')\n",
    "ax.set_ylabel('A.U.')\n",
    "ax.set_title('Jet Multiplicity Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6352bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pileup distribution\n",
    "bins = 20\n",
    "x_min = 0\n",
    "x_max = nPU.max().item() + 1\n",
    "fig, ax = plt.subplots()\n",
    "sig_pu = ak.to_numpy(sig['Pileup_nPU'])\n",
    "bkg_pu = ak.to_numpy(bkg['Pileup_nPU'])\n",
    "ax.hist(sig_pu, bins=bins, range=(x_min, x_max), density=True, label='Signal (EMJ)', histtype='step')\n",
    "ax.hist(bkg_pu, bins=bins, range=(x_min, x_max), density=True, label='Background (QCD)', histtype='step')\n",
    "ax.set_xlabel('Number of Pileup')\n",
    "ax.set_ylabel('A.U.')\n",
    "ax.set_title('Pileup Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pt distrib using awkward arrays\n",
    "# Plot njets distribution\n",
    "sig_njets = ak.to_numpy(sig['nJet'])\n",
    "bkg_njets = ak.to_numpy(bkg['nJet'])\n",
    "sig_jets = sig[sig_njets > njets]\n",
    "bkg_jets = bkg[bkg_njets > njets]\n",
    "\n",
    "def plot_ftrs(sig, bkg, ftr_name, fig_title, nbins, xrange, xlabel, ylabel, log, normalized=True):\n",
    "    sig_ftr = sig[ftr_name]\n",
    "    bkg_ftr = bkg[ftr_name]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(ak.flatten(sig_ftr), bins=nbins, range=xrange, density=normalized, alpha=0.5, label='EMJ', histtype='step', linewidth=1.5)\n",
    "    ax.hist(ak.flatten(bkg_ftr), bins=nbins, range=xrange, density=normalized, alpha=0.5, label='QCD', histtype='step', linewidth=1.5)\n",
    "    ax.set_title(fig_title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_yscale('log' if log else 'linear')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687df949",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_pt\", \"Jet $p_T$ Distribution\", 20, (0, 2000), \"Jet $p_T$ [GeV]\", \"Count\", log=True)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_eta\", \"Jet $\\eta$ Distribution\", 20, (-5, 5), \"Jet $\\eta$\", \"Count\", log=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_phi\", \"Jet $\\phi$ Distribution\", 20, (-3.14, 3.14), \"Jet $\\phi$\", \"Count\", log=False)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_mass\", \"Jet Mass Distribution\", 20, (0, 250), \"Jet Mass [GeV]\", \"Count\", log=True)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_nConstituents\", \"Jet Number of Constituents Distribution\", 20, (0, 20), \"Number of Constituents\", \"Count\", log=True)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_nSVs\", \"Jet Number of Secondary Vertices Distribution\", 20, (0, 20), \"Number of Secondary Vertices\", \"Count\", log=True)\n",
    "plot_ftrs(sig_jets, bkg_jets, \"Jet_area\", \"Jet Area Distribution\", 30, (0, 1), \"Jet Area\", \"Count\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at feature correlation with PU\n",
    "# Use torch.corrcoef\n",
    "\n",
    "lead_jets = data_tensor[:, 0, :]\n",
    "corr_coefs = {}\n",
    "for i, ftr_name in enumerate(features):\n",
    "    corr_matrix = torch.corrcoef(torch.stack((lead_jets[:, i], nPU), dim=0))\n",
    "    corr_coefs[ftr_name] = corr_matrix[0, 1].item()\n",
    "    print(f\"Correlation coefficient between {ftr_name} and nPU: {corr_coefs[ftr_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198322f",
   "metadata": {},
   "source": [
    "## Model Instantiation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3766a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_epochs\": 20,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 512,\n",
    "    \"num_heads\": 2,\n",
    "    \"num_layers\": 1,\n",
    "    \"hidden_size\": 4,\n",
    "    \"infonce_temp\": 0.07,\n",
    "    \"beta\": 0.5\n",
    "}\n",
    "pprint(config)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "encoder = model.TransformerEncoder(\n",
    "    num_features=num_ftrs,\n",
    "    embed_size=config[\"hidden_size\"],\n",
    "    num_heads=config[\"num_heads\"],\n",
    "    num_layers=config[\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "classifier = model.MLPClassifier(\n",
    "    input_dim=config[\"hidden_size\"],\n",
    "    hidden_dim=config[\"hidden_size\"],\n",
    "    hidden_layers=2,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "infonce_loss = training.InfoNCELoss(config[\"infonce_temp\"])\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(classifier.parameters()),\n",
    "    lr=config[\"learning_rate\"]\n",
    ")\n",
    "metrics = metrics.metrics()\n",
    "\n",
    "print(\"\")\n",
    "pprint(\"Classifier arch:\")\n",
    "pprint(classifier)\n",
    "print(\"\")\n",
    "pprint(\"Encoder arc:\")\n",
    "pprint(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e073192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "wandb.init(\n",
    "    project=\"ml4phys_finalproj\",\n",
    "    name=f\"run_encoderclassifier_{timestamp}\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "for epoch in range(config[\"n_epochs\"]):\n",
    "    metrics.reset()\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "\n",
    "    correct = count = 0\n",
    "    total_loss = total_bce = total_infonce = 0.0\n",
    "    for x, mask, labels in train_loader:\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        latent = encoder(x, mask).to(device)\n",
    "        logit = classifier(latent)\n",
    "\n",
    "        loss_infonce = infonce_loss(latent, labels)\n",
    "        loss_bce = bce_loss(logit.squeeze(), labels)\n",
    "        loss = config[\"beta\"] * loss_bce + (1 - config[\"beta\"]) * loss_infonce\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics.update(\n",
    "            labels.cpu(), \n",
    "            (sigmoid(logit).squeeze() > 0.5).long().cpu()\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_bce += loss_bce.item() * x.size(0)\n",
    "        total_infonce += loss_infonce.item() * x.size(0)\n",
    "        \n",
    "    avg_loss = total_loss / len(train_ds)\n",
    "    avg_bce = total_bce / len(train_ds)\n",
    "    avg_infonce = total_infonce / len(train_ds)\n",
    "    accuracy, precision, recall, f1_score = metrics.compute()\n",
    "\n",
    "    # Eval\n",
    "    metrics.reset()\n",
    "    correct_val = count_val = 0\n",
    "    total_loss_val = total_bce_val = total_infonce_val = 0.0\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_val, mask_val, labels_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "\n",
    "            latent_val = encoder(x_val, mask_val).to(device)\n",
    "            logit_val = classifier(latent_val)\n",
    "\n",
    "            loss_infonce_val = infonce_loss(latent_val, labels_val)\n",
    "            loss_bce_val = bce_loss(logit_val.squeeze(), labels_val)\n",
    "            loss_val = config[\"beta\"] * loss_bce_val + (1 - config[\"beta\"]) * loss_infonce_val\n",
    "\n",
    "            total_loss_val += loss_val.item() * x_val.size(0)\n",
    "            total_bce_val += loss_bce_val.item() * x_val.size(0)\n",
    "            total_infonce_val += loss_infonce_val.item() * x_val.size(0)\n",
    "\n",
    "        metrics.update(labels_val.cpu(), (sigmoid(logit_val).squeeze() > 0.5).long().cpu())\n",
    "\n",
    "    avg_loss_val = total_loss_val / len(val_ds)\n",
    "    avg_bce_val = total_bce_val / len(val_ds)\n",
    "    avg_infonce_val = total_infonce_val / len(val_ds)\n",
    "    accuracy_val, precision_val, recall_val, f1_score_val = metrics.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config['n_epochs']}\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, BCE: {avg_bce:.4f}, InfoNCE: {avg_infonce:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Val Loss: {avg_loss_val:.4f}, Val BCE: {avg_bce_val:.4f}, Val InfoNCE: {avg_infonce_val:.4f}, Val Accuracy: {accuracy_val:.4f}, Val Precision: {precision_val:.4f}, Val Recall: {recall_val:.4f}, Val F1 Score: {f1_score_val:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"loss\": avg_loss,\n",
    "        \"bce_loss\": avg_bce,\n",
    "        \"infonce_loss\": avg_infonce,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"val_loss\": avg_loss_val,\n",
    "        \"val_bce_loss\": avg_bce_val,\n",
    "        \"val_infonce_loss\": avg_infonce_val,\n",
    "        \"val_accuracy\": accuracy_val,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"val_precision\": precision_val,\n",
    "        \"val_recall\": recall_val,\n",
    "        \"val_f1_score\": f1_score_val,\n",
    "    })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "true_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "metrics.reset()\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, mask_test, label_test in test_loader:\n",
    "        x_test = x_test.to(device)\n",
    "        mask_test = mask_test.to(device)\n",
    "        label_test = label_test.to(device)\n",
    "\n",
    "        latent_test = encoder(x_test, mask_test)\n",
    "        logit_test = classifier(latent_test)\n",
    "        probs = sigmoid(logit_test).squeeze()\n",
    "        preds = (probs > 0.5).long()\n",
    "\n",
    "        metrics.update(label_test.cpu(), preds.cpu())\n",
    "        true_labels.extend(label_test.cpu().tolist())\n",
    "        predicted_probs.extend(probs.cpu().tolist())\n",
    "\n",
    "accuracy_test, precision_test, recall_test, f1_score_test = metrics.compute()\n",
    "print(f\"Test Accuracy: {accuracy_test:.4f}, Test Precision: {precision_test:.4f}, Test Recall: {recall_test:.4f}, Test F1 Score: {f1_score_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_roc(true_labels, predicted_probs)\n",
    "utils.plot_scores(\n",
    "    sig_scores=[pred for pred, label in zip(predicted_probs, true_labels) if label == 1],\n",
    "    bkg_scores=[pred for pred, label in zip(predicted_probs, true_labels) if label == 0],\n",
    "    bins=50,\n",
    "    range=(0,1),\n",
    "    logscale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59c7b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a93c84",
   "metadata": {},
   "source": [
    "Junk code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-bin feature correlation with PU\n",
    "PU_bins = torch.linspace(0, nPU.max().item()+1, steps=51)\n",
    "per_feature_bin_corrs = [] # Eventual shape: (num_features, num_bins)\n",
    "lead_jet_ftrs = data_tensor[:, 0, :]\n",
    "for i, ftr_name in enumerate(features):\n",
    "    bin_corrs = []\n",
    "    for j in range(len(PU_bins)-1):\n",
    "        bin_mask = (nPU >= PU_bins[j]) & (nPU < PU_bins[j+1])\n",
    "        if bin_mask.sum() < 2:\n",
    "            bin_corrs.append(0.0)\n",
    "            continue\n",
    "        bin_ftr_values = lead_jet_ftrs[bin_mask, i]\n",
    "        bin_nPU_values = nPU[bin_mask]\n",
    "        corr_matrix = torch.corrcoef(torch.stack((bin_ftr_values, bin_nPU_values), dim=0))\n",
    "        bin_corrs.append(corr_matrix[0, 1].item())\n",
    "    per_feature_bin_corrs.append(bin_corrs)\n",
    "per_feature_bin_corrs = torch.tensor(per_feature_bin_corrs)\n",
    "print(per_feature_bin_corrs.shape)\n",
    "\n",
    "# Heat map where x-axis is PU bins, y-axis is features, color is correlation coefficient\n",
    "# Make them two plots: top = nPU histogram, bottom = heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cax = ax.imshow(per_feature_bin_corrs.numpy(), aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_xticks(np.arange(len(PU_bins)-1))\n",
    "ax.set_xticklabels([f\"{PU_bins[i]:.1f}-{PU_bins[i+1]:.1f}\" for i in range(len(PU_bins)-1)], rotation=45)\n",
    "ax.set_yticks(np.arange(len(features)))\n",
    "ax.set_yticklabels([ftr.replace(\"Jet_\", \"\") for ftr in features])\n",
    "ax.set_xlabel('nPU Bins')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Feature Correlation with Pileup across nPU Bins')\n",
    "fig.colorbar(cax, label='Correlation Coefficient')\n",
    "plt.show()\n",
    "\n",
    "# Plot nPU histogram\n",
    "plt.hist(nPU.numpy(), bins=PU_bins.numpy(), alpha=0.7, color='blue', histtype='step')\n",
    "plt.xlabel('Number of Primary Vertices (nPU)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of nPU')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
